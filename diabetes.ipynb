{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = r\"datasets/diabetes.csv\"\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt(d, delimiter=',', dtype=np.float32)\n",
    "\n",
    "# features \n",
    "x_data = Variable(torch.from_numpy(xy[:,0:-1]))\n",
    "# label\n",
    "y_data = Variable(torch.from_numpy(xy[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # 8 inputs 6 outputs\n",
    "        self.l1 = torch.nn.Linear(8, 6)\n",
    "        # 6 inputs 4 outputs\n",
    "        self.l2 = torch.nn.Linear(6, 4)\n",
    "        # 4 inputs 1 output - binary classification\n",
    "        self.l3 = torch.nn.Linear(4, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out1 = F.relu(self.l1(x))\n",
    "        out2 = F.relu(self.l2(out1))\n",
    "        y_prediction = self.sigmoid(self.l3(out2))\n",
    "        return y_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 \tloss:  tensor(0.8363, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  1 \tloss:  tensor(0.7128, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  2 \tloss:  tensor(0.6950, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  3 \tloss:  tensor(0.6887, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  4 \tloss:  tensor(0.6820, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  5 \tloss:  tensor(0.6752, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  6 \tloss:  tensor(0.6670, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  7 \tloss:  tensor(0.6582, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  8 \tloss:  tensor(0.6491, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  9 \tloss:  tensor(0.6398, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  10 \tloss:  tensor(0.6306, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  11 \tloss:  tensor(0.6240, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  12 \tloss:  tensor(0.6209, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  13 \tloss:  tensor(0.6195, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  14 \tloss:  tensor(0.6186, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  15 \tloss:  tensor(0.6214, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  16 \tloss:  tensor(0.6215, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  17 \tloss:  tensor(0.6389, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  18 \tloss:  tensor(0.6116, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  19 \tloss:  tensor(0.6104, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  20 \tloss:  tensor(0.6109, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  21 \tloss:  tensor(0.6131, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  22 \tloss:  tensor(0.6121, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  23 \tloss:  tensor(0.6212, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  24 \tloss:  tensor(0.6206, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  25 \tloss:  tensor(0.6439, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  26 \tloss:  tensor(0.6080, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  27 \tloss:  tensor(0.6081, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  28 \tloss:  tensor(0.6107, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  29 \tloss:  tensor(0.6187, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  30 \tloss:  tensor(0.6382, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  31 \tloss:  tensor(0.6069, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  32 \tloss:  tensor(0.6075, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  33 \tloss:  tensor(0.6084, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  34 \tloss:  tensor(0.6178, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  35 \tloss:  tensor(0.6174, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  36 \tloss:  tensor(0.6409, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  37 \tloss:  tensor(0.6048, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  38 \tloss:  tensor(0.6042, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  39 \tloss:  tensor(0.6046, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  40 \tloss:  tensor(0.6086, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  41 \tloss:  tensor(0.6177, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  42 \tloss:  tensor(0.6447, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  43 \tloss:  tensor(0.6046, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  44 \tloss:  tensor(0.6028, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  45 \tloss:  tensor(0.6026, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  46 \tloss:  tensor(0.6050, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  47 \tloss:  tensor(0.6111, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  48 \tloss:  tensor(0.6345, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  49 \tloss:  tensor(0.6042, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  50 \tloss:  tensor(0.6117, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  51 \tloss:  tensor(0.6215, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  52 \tloss:  tensor(0.6536, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  53 \tloss:  tensor(0.6068, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  54 \tloss:  tensor(0.6071, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  55 \tloss:  tensor(0.6192, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  56 \tloss:  tensor(0.6076, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  57 \tloss:  tensor(0.6210, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  58 \tloss:  tensor(0.6057, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  59 \tloss:  tensor(0.6151, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  60 \tloss:  tensor(0.6131, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  61 \tloss:  tensor(0.6307, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  62 \tloss:  tensor(0.6011, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  63 \tloss:  tensor(0.6002, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  64 \tloss:  tensor(0.5995, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  65 \tloss:  tensor(0.6003, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  66 \tloss:  tensor(0.6012, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  67 \tloss:  tensor(0.6125, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  68 \tloss:  tensor(0.6209, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  69 \tloss:  tensor(0.6627, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  70 \tloss:  tensor(0.6102, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  71 \tloss:  tensor(0.6062, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  72 \tloss:  tensor(0.6149, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  73 \tloss:  tensor(0.6073, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  74 \tloss:  tensor(0.6201, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  75 \tloss:  tensor(0.6028, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  76 \tloss:  tensor(0.6070, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  77 \tloss:  tensor(0.6081, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  78 \tloss:  tensor(0.6218, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  79 \tloss:  tensor(0.6015, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  80 \tloss:  tensor(0.6062, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  81 \tloss:  tensor(0.6094, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  82 \tloss:  tensor(0.6275, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  83 \tloss:  tensor(0.5987, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  84 \tloss:  tensor(0.5975, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  85 \tloss:  tensor(0.5969, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  86 \tloss:  tensor(0.5976, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  87 \tloss:  tensor(0.5996, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  88 \tloss:  tensor(0.6123, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  89 \tloss:  tensor(0.6232, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  90 \tloss:  tensor(0.6639, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  91 \tloss:  tensor(0.6126, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  92 \tloss:  tensor(0.6008, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  93 \tloss:  tensor(0.6001, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  94 \tloss:  tensor(0.6014, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  95 \tloss:  tensor(0.6136, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  96 \tloss:  tensor(0.6070, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  97 \tloss:  tensor(0.6202, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  98 \tloss:  tensor(0.5984, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch:  99 \tloss:  tensor(0.6017, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(\"epoch: \", epoch, \"\\tloss: \", loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
